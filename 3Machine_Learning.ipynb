{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_df = pd.read_csv(\"state_headlines_for_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_df[\"headline_raw\"] = head_df[\"headline.main\"].str.lower().replace(\n",
    "    \"[^A-Za-z\\s]\", \"\").str.replace(\";\", \"\").replace(\",\", \"\")\n",
    "\n",
    "head_df[\"head_split\"] = head_df[\"headline_raw\"].str.split(\",\").str.join(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(norm=False)\n",
    "vec.fit(head_df[\"head_split\"])\n",
    "X_train = vec.transform(head_df[\"head_split\"])\n",
    "y_train = head_df[\"state\"]\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "model = KNeighborsClassifier(metric=\"euclidean\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "pipeline = Pipeline([(\"vectorizer\", vec), (\"scaler\", scaler), (\"fit\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal K:  2\n"
     ]
    }
   ],
   "source": [
    "# calculates estimate of test error based on 5-fold cross validation\n",
    "def get_cv_error(k):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    pipeline = Pipeline([(\"vectorizer\", vec), (\"scaler\", scaler), (\"fit\", model)])\n",
    "    f1 = np.mean(-cross_val_score(\n",
    "        pipeline, head_df[\"head_split\"], y_train == \"Colorado\", \n",
    "        cv=10, scoring=\"f1\"\n",
    "    ))\n",
    "    return f1\n",
    "    \n",
    "ks = pd.Series(range(1, 25))\n",
    "ks.index = range(1, 25)\n",
    "test_errs = ks.apply(get_cv_error)\n",
    "best_k = test_errs.sort_values(ascending=False).idxmax()\n",
    "print(\"Optimal K: \", best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1b55fbba90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errs.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update model to get optimal K\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=best_k, metric=\"euclidean\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "pipeline = Pipeline([(\"vectorizer\", vec), (\"scaler\", scaler), (\"fit\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy:\n",
    "my_accuracy = cross_val_score(pipeline, head_df[\"head_split\"], y_train, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "f1_list = []\n",
    "\n",
    "for state in y_train.unique():\n",
    "    recall_list.append((cross_val_score(pipeline, head_df[\"head_split\"], y_train == state, cv=10, scoring=\"recall\").mean()))\n",
    "    precision_list.append(cross_val_score(pipeline, head_df[\"head_split\"], y_train == state, cv=10, scoring=\"precision\").mean())\n",
    "    f1_list.append(cross_val_score(pipeline, head_df[\"head_split\"], y_train == state, cv=10, scoring=\"f1\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accuracy=np.mean(my_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_list = [precision_list, recall_list, f1_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(combo_list).rename({0:head_df[\"state\"].unique()[0],\n",
    "                                1: head_df[\"state\"].unique()[1],\n",
    "                                2: head_df[\"state\"].unique()[2],\n",
    "                                3: head_df[\"state\"].unique()[3],\n",
    "                                4: head_df[\"state\"].unique()[4]},\n",
    "                               axis=1).rename(index={0:\"Precision\", 1:\"Recall\", 2:\"F1_score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>0.822381</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.470368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>0.844048</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.644873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.658067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>0.863095</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.551543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.329487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Precision  Recall  F1_score\n",
       "Colorado      0.822381    0.34  0.470368\n",
       "New York      0.844048    0.54  0.644873\n",
       "Washington    0.897500    0.53  0.658067\n",
       "Mississippi   0.863095    0.42  0.551543\n",
       "Texas         0.783333    0.21  0.329487"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59599999999999997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fair to say that our model is pretty high-performing when it comes to finding out which state a headline was written about. The fact that our model was able to distinguish between which headlines were written with respect with each state may be evidence that the five groups of headlines were significantly different, lending some credibility to the point of view that NYT has been writing differently about some states than others in terms of word choice and, by proxy, tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For two more unrelated but high quality machine learning models we made throughout the course of this project, please check out our notebook titled \"Extra_Work\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
